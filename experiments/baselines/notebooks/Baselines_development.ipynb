{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df79b402b4d4b6785908e16c129531e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "37.318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fd8c1b189d434691352cabf4a2548d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "21.92962\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a9fcd38ee740dd8356efb257d181c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "13.00882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52b68ab23734609a40589160fadaa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "16.357058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f09518f5124422a964ec528e8fcc948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "24.835165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f061f0ccb541e2972496839e8a6eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "12.250585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d629e24eca47acae70468dc490b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "18.062805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8099be8733a4446ae41054917732d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "11.551099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b847a5ab30e468586813833a97e05e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "30.56658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c190b05afad4d5c9b6119fc824a6f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6]) torch.Size([30, 6])\n",
      "46.886597\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model_name = sys.argv[0].split('/')[-1].replace('.py','')\n",
    "path = '../../../data_and_results/u-air/production/pm25_beijing_best36/quadratic/'\n",
    "fold = '0'\n",
    "for f_id in [str(i).zfill(5) for i in range(10)]:\n",
    "    \n",
    "    trn_X = torch.tensor(np.load(path+'data/fold_'+fold+'/train/X/'+f_id+'.npz')['arr_0'], dtype=torch.float32)\n",
    "    trn_y = torch.tensor(np.load(path+'data/fold_'+fold+'/train/y/'+f_id+'.npz')['arr_0'], dtype=torch.float32)\n",
    "    tst_X = torch.tensor(np.load(path+'data/fold_'+fold+'/test/X/'+f_id+'.npz')['arr_0'], dtype=torch.float32)\n",
    "    tst_y = torch.tensor(np.load(path+'data/fold_'+fold+'/test/y/'+f_id+'.npz')['arr_0'], dtype=torch.float32)\n",
    "    data_dim = trn_X.size(-1)\n",
    "    n_dim = 2\n",
    "\n",
    "    class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "        def __init__(self):\n",
    "            super(LargeFeatureExtractor, self).__init__()\n",
    "            self.add_module('linear1', torch.nn.Linear(data_dim, 64))\n",
    "            self.add_module('relu1', torch.nn.ReLU())\n",
    "            self.add_module('linear2', torch.nn.Linear(64, 32))\n",
    "            self.add_module('relu2', torch.nn.ReLU())\n",
    "            self.add_module('linear3', torch.nn.Linear(32, 16))\n",
    "            self.add_module('relu3', torch.nn.ReLU())\n",
    "            self.add_module('linear4', torch.nn.Linear(16, n_dim))\n",
    "\n",
    "    feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "    scaler = pd.read_pickle(path+'data/fold_'+fold+'/scaler/'+f_id+'.pickle')\n",
    "\n",
    "    class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "#             self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "#                 gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=n_dim)),\n",
    "#                 num_dims=n_dim, grid_size=100\n",
    "#             )\n",
    "#             print(gpytorch.utils.grid.choose_grid_size(train_x))\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=n_dim))\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(x)\n",
    "#           Min-Max scaling\n",
    "#             projected_x = projected_x - projected_x.min(0)[0]\n",
    "#             projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "#           Standard scaling\n",
    "#             print('before', type(projected_x), projected_x.shape)\n",
    "#             print(projected_x.mean(axis=0))\n",
    "            projected_x = projected_x - projected_x.mean(axis=0)\n",
    "            projected_x = projected_x/projected_x.std(axis=0)\n",
    "#             print('after', type(projected_x), projected_x.shape)\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(trn_X, trn_y.ravel(), likelihood)\n",
    "    training_iterations = 200\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters()},\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.01)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    def train():\n",
    "        iterator = tqdm.notebook.tqdm(range(training_iterations))\n",
    "        losses = []\n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(trn_X)\n",
    "    #         print(output.shape, type(output), output)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, trn_y.ravel())\n",
    "#             print(1 if loss>np.inf else 0)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            optimizer.step()\n",
    "        return losses\n",
    "\n",
    "    losses = train()\n",
    "    print(tst_X.shape, trn_X.shape)    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "        preds = model(tst_X).mean.numpy()\n",
    "\n",
    "    pred_y = scaler.inverse_transform(preds)\n",
    "    print(mean_absolute_error(tst_y.ravel(), pred_y))\n",
    "    # if not os.path.exists(path+'results/'+model_name+'/fold_'+fold+'/'):\n",
    "    #     os.makedirs(path+'results/'+model_name+'/fold_'+fold+'/')\n",
    "\n",
    "    # np.savez_compressed(path+'results/'+model_name+'/fold_'+fold+'/'+f_id+'.npz', pred_y)\n",
    "    # pd.to_pickle(model.param_array, path+'results/'+model_name+'/fold_'+fold+'/'+f_id+'.model')\n",
    "#     plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2453460301567\n",
      "17.016735544217056\n",
      "16.74793164514159\n",
      "17.09705504922577\n",
      "22.799159167430588\n",
      "20.258048564615247\n",
      "21.241996485071795\n",
      "30.25906078389399\n",
      "44.77660282120676\n",
      "39.258164003554825\n"
     ]
    }
   ],
   "source": [
    "opath = '../../../data_and_results/u-air/production/pm25_beijing_best36/quadratic/results/gp_rbf/fold_0/'\n",
    "tpath = '../../../data_and_results/u-air/production/pm25_beijing_best36/quadratic/data/fold_0/'\n",
    "for i in [str(i).zfill(5) for i in range(10)]:\n",
    "    pred = np.load(opath+i+'.npz')['arr_0']\n",
    "    tst = np.load(tpath+'test/y/'+i+'.npz')['arr_0']\n",
    "    print(mean_absolute_error(tst, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([216.42017, 218.27953, 207.00104, 251.74893, 244.90967, 348.6796 ],\n",
       "       dtype=float32),\n",
       " tensor([[377.],\n",
       "         [224.],\n",
       "         [259.],\n",
       "         [218.],\n",
       "         [228.],\n",
       "         [306.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y, tst_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
