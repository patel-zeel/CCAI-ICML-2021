{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/patel-zeel/Adhoc/master/ADAIN.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9e6b922ca042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# !pip install -qq numpy==1.18.5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Concatenate, Lambda, Multiply, Reshape, Input, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from tensorflow.random import set_seed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import clear_output\n",
    "from time import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "from kgp.layers import GP\n",
    "from kgp.models import Model\n",
    "from kgp.losses import gen_gp_loss\n",
    "\n",
    "rc('font', size=16)\n",
    "# import logging\n",
    "# tf.get_logger().setLevel(logging.ERROR)\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAIN(time_window, met_dim):\n",
    "    \"\"\"\n",
    "    # Vocab\n",
    "    met         - meteorology\n",
    "    dis         - distance\n",
    "    aq          - air quality\n",
    "    n_stn       - n_stations\n",
    "    dim         - dimension\n",
    "    \"\"\"\n",
    "    with tf.device('gpu:1'):\n",
    "        # Hyperparameters\n",
    "        do = 0.5 # Drop out\n",
    "        np.random.seed(0)\n",
    "        set_seed(0)\n",
    "\n",
    "        # Defining inputs\n",
    "        train_dist = Input(shape=(2,None,))\n",
    "        train_met_aq = Input(shape=(24,met_dim+1,None,)) # +1 for aq\n",
    "        test_met = Input(shape=(24,met_dim,))\n",
    "\n",
    "        n_stn = 2 if train_dist.shape[2] is None else train_dist.shape[2]\n",
    "\n",
    "        # Local station                                                        ## Input                --- Output\n",
    "        lcl_lstm = LSTM(300, dropout=do)(test_met)                             # (-1,time_window,met)  --- (-1,300)\n",
    "        lcl_dns = Dense(200, activation='relu')(lcl_lstm)                      # (-1,300)              --- (-1,200) \n",
    "        lcl_dns = Dropout(do)(lcl_dns)\n",
    "\n",
    "        # Defininig shared layers (shared among train stations)\n",
    "        s_dns1 = Dense(100, activation='relu')\n",
    "        s_lstm = LSTM(300, activation='relu', dropout=do)\n",
    "        s_dns2 = Dense(200, activation='relu')\n",
    "\n",
    "        a_dns1 = Dense(200, activation='relu')\n",
    "        a_dns2 = Dense(200)\n",
    "        a_dns3 = Dense(1)\n",
    "\n",
    "        # Training stations\n",
    "        att_list = [] # Saving station attention weights\n",
    "        stn_list = [] # Saving station features\n",
    "\n",
    "        # Iterating over train stations using shared layers\n",
    "        for s_i in range(n_stn):                                               ## Input               --- Output\n",
    "            train_dist_slice = Lambda(lambda x:x[:,:,s_i])(train_dist)         # (-1,dis,n_stn)       --- (-1,dis)\n",
    "            stn_dns1 = s_dns1(train_dist_slice)                                # (-1,dis)             --- (-1,100)\n",
    "            stn_dns1 = Dropout(do)(stn_dns1)\n",
    "            train_met_aq_slice = Lambda(lambda x:x[:,:,:,s_i])(train_met_aq)\n",
    "            stn_lstm = s_lstm(train_met_aq_slice)                              # (-1,time_window,met) --- (-1,300)\n",
    "            stn_cat1 = Concatenate()([stn_dns1, stn_lstm])                     # (-1,100)+(-1,300)    --- (-1,400)\n",
    "            stn_dns2 = s_dns2(stn_cat1)                                        # (-1,400)             --- (-1,200)\n",
    "            stn_dns2 = Dropout(do)(stn_dns2)\n",
    "            stn_list.append(stn_dns2)\n",
    "            ### Attention Mechanism\n",
    "            att_cat = Concatenate()([stn_dns2, lcl_dns])                       # (-1,200)*2           --- (-1,400)\n",
    "            att_dns1 = a_dns1(att_cat)                                         # (-1,400)             --- (-1,200)\n",
    "            att_dns1 = Dropout(do)(att_dns1)\n",
    "            att_dns2 = a_dns2(att_dns1)                                        # (-1,200)             --- (-1,200)\n",
    "            att_dns3 = a_dns3(att_dns2)                                        # (-1,200)             --- (-1,1)\n",
    "            att_list.append(att_dns3)\n",
    "\n",
    "        ### Normalize Attention\n",
    "        att_cat1 = Concatenate()(att_list)                                     # (-1,1)*n_stn         --- (-1,n_stn)\n",
    "        att_cat2 = Lambda(lambda inp: inp/K.sum(inp, axis=0))(att_cat1)        # (-1,n_stn)           --- (-1,n_stn)\n",
    "\n",
    "        ### Multiply Attention with station features\n",
    "        stn_cat2 = Concatenate()(stn_list)                                     # (-1,200)*n_station   --- (-1,200*n_stn) \n",
    "        stn_cat2 = Lambda(lambda x: K.reshape(x, (-1,200,n_stn)))(stn_cat2)    # (-1,200*n_stn)       --- (-1,200,n_stn)      \n",
    "        att_cat2 = Lambda(lambda x: K.reshape(x, (-1,1,n_stn)))(att_cat2)      # (-1,n_stn)           --- (-1,1,n_stn)\n",
    "    #     print(stn_cat2)\n",
    "        stn_mul = Multiply()([stn_cat2, att_cat2])                             # (-1,200,n_stn)*(-1,1,n_stn)  --- (-1,200,n_stn)\n",
    "        stn_add = Lambda(lambda x: K.sum(x, axis=2))(stn_mul)                  # (-1,200,n_stn)       --- (-1,200)\n",
    "\n",
    "        ### Concatenate local and station features\n",
    "        final_cat = Concatenate()([stn_add, lcl_dns])                          # (-1,200)*2           --- (-1,400)\n",
    "        final_dns1 = Dense(200, activation='relu')(final_cat)                  # (-1,400)             --- (-1,200)\n",
    "        final_dns1 = Dropout(do)(final_dns1)\n",
    "        final_dns2 = Dense(200)(final_dns1)\n",
    "        final_dns3 = Dense(1)(final_dns2)                                      # (-1,200)             --- (-1,1)\n",
    "\n",
    "        batch_size = 32\n",
    "        nb_train_samples = 7620\n",
    "        gp_hypers = {'lik': -2.0, 'cov':np.random.rand(200,1)}\n",
    "\n",
    "        gp = GP(gp_hypers,\n",
    "            batch_size=batch_size,\n",
    "            nb_train_samples=nb_train_samples)\n",
    "        outputs = [gp(final_dns3)]\n",
    "        model = Model(inputs=[train_dist, train_met_aq, test_met], outputs=outputs)\n",
    "\n",
    "        # Compile the model\n",
    "        loss = [gen_gp_loss(gp) for gp in model.layers]\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(1e-2), loss=loss)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "(7620, 2, 29)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "set_seed(0)\n",
    "path = '../../../data_and_results/u-air/production/pm25_beijing_best36/quadratic/'\n",
    "n_folds = 6\n",
    "\n",
    "init = time()\n",
    "fold_history = []\n",
    "for fold in [str(i) for i in range(n_folds)]:\n",
    "    clear_output(wait=True)\n",
    "    print('Training fold',fold)\n",
    "    # Load data\n",
    "    train_dst = np.load(path+'data/fold_'+fold+'/train/adain/trn_dst.npy', allow_pickle=True).astype(np.float32)\n",
    "    train_met_aq = np.load(path+'data/fold_'+fold+'/train/adain/trn_metaq.npy').astype(np.float32)\n",
    "    test_met = np.load(path+'data/fold_'+fold+'/train/adain/tst_met.npy').astype(np.float32)\n",
    "    test_aq = np.load(path+'data/fold_'+fold+'/train/adain/tst_aqi.npy').astype(np.float32)\n",
    "    yscaler = pd.read_pickle(path+'data/fold_'+fold+'/scaler/adain/yscaler.pickle')\n",
    "    print(train_dst.shape)\n",
    "    # Define and train the model\n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2, restore_best_weights=True)\n",
    "    model = ADAIN(time_window=24, met_dim=4)\n",
    "    history = model.fit(X=[train_dst, train_met_aq, test_met], Y=test_aq, \n",
    "          batch_size=32, epochs=10, validation_split=0.1, verbose=2)#, callbacks=[es])\n",
    "    fold_history.append(history)\n",
    "    \n",
    "    # Test the model\n",
    "    train_dst = np.load(path+'data/fold_'+fold+'/test/adain/trn_dst.npy', allow_pickle=True).astype(np.float32)\n",
    "    train_met_aq = np.load(path+'data/fold_'+fold+'/test/adain/trn_metaq.npy').astype(np.float32)\n",
    "    test_met = np.load(path+'data/fold_'+fold+'/test/adain/tst_met.npy').astype(np.float32)\n",
    "    test_aq = np.load(path+'data/fold_'+fold+'/test/adain/tst_aqi.npy').astype(np.float32)\n",
    "    \n",
    "    pred_y = np.nan*np.zeros((train_dst.shape[0], train_dst.shape[-1]))\n",
    "    for test_id in range(train_dst.shape[-1]):\n",
    "        pred_yy = model.predict(x=[train_dst[:,:,:,test_id], train_met_aq, test_met[:,:,:,test_id]])\n",
    "        pred_y[:,test_id] = yscaler.inverse_transform(pred_yy).ravel()\n",
    "    \n",
    "    if not os.path.exists(path+'results/adain/fold_'+fold):\n",
    "        os.makedirs(path+'results/adain/fold_'+fold)\n",
    "    np.save(path+'results/adain/fold_'+fold+'/pred_y.npy', pred_y)\n",
    "    np.save(path+'results/adain/fold_'+fold+'/test_y.npy', test_aq)\n",
    "\n",
    "print('Finished in',round(time()-init)/60,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data_and_results/u-air/production/pm25_beijing_best36/quadratic/'\n",
    "folds = [str(i) for i in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, len(folds)//2, figsize=(15,6))\n",
    "ax = ax.ravel()\n",
    "for i in range(len(folds)):\n",
    "    ax[i].plot(fold_history[i].history['loss'], label='train loss')\n",
    "    ax[i].plot(fold_history[i].history['val_loss'], label='validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['svr', 'gp_rbf', 'gp_m32', 'gp_m12', 'gp_linear','nsgp_rbf','adain']\n",
    "res = pd.DataFrame(index=models, columns=['fold_'+str(i) for i in range(len(folds))]+['avg'])\n",
    "# for model in models[:-1]:\n",
    "#     pred_y, test_y = load_results(path, folds, f_ids, n_test, model)\n",
    "#     fold_rmse = fold_wise_rmse(pred_y, test_y, len(folds))\n",
    "#     fold_rmse.append(np.mean(fold_rmse))\n",
    "#     res.loc[model, :] = fold_rmse\n",
    "\n",
    "# ADAIN\n",
    "for fold in folds:\n",
    "    pred_y = np.load(path+'results/adain/fold_'+fold+'/pred_y.npy')\n",
    "    test_y = np.load(path+'results/adain/fold_'+fold+'/test_y.npy')\n",
    "    res.loc['adain', 'fold_'+fold] = mean_squared_error(test_y.ravel(), pred_y.ravel(), squared=False)\n",
    "res.loc['adain','avg'] = res.loc['adain'].mean()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting predictions on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_aq.ravel(), test_aq.ravel())\n",
    "plt.scatter(test_aq.ravel(), pred_y.ravel());\n",
    "plt.xlabel('PM2.5 (Ground truth)');plt.ylabel('PM2.5 (Predictions)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(o_tests, o_tests)\n",
    "plt.plot(test_aq[:,0], label='ground truth');\n",
    "plt.plot(pred_y[:,0], label='predictions');\n",
    "plt.xlabel('Time-stamps'); plt.ylabel('PM2.5');\n",
    "plt.legend(bbox_to_anchor=(1.5,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-565cd2cc971f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m gp = GP(gp_hypers,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Keras-2.1.3-py3.8.egg/keras/engine/topology.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m     input_layer = InputLayer(batch_input_shape=batch_shape,\n\u001b[0m\u001b[1;32m   1453\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Keras-2.1.3-py3.8.egg/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Keras-2.1.3-py3.8.egg/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/Keras-2.1.3-py3.8.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from kgp.layers import GP\n",
    "from kgp.models import Model\n",
    "from kgp.losses import gen_gp_loss\n",
    "\n",
    "input_shape = (10, 2)  # 10 time steps, 2 dimensions\n",
    "batch_size = 32\n",
    "nb_train_samples = 512\n",
    "gp_hypers = {'lik': -2.0, 'cov': [[-0.7], [0.0]]}\n",
    "\n",
    "# Build the model\n",
    "inputs = Input(shape=input_shape)\n",
    "rnn = SimpleRNN(32)(inputs)\n",
    "gp = GP(gp_hypers,\n",
    "        batch_size=batch_size,\n",
    "        nb_train_samples=nb_train_samples)\n",
    "outputs = [gp(rnn)]\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "loss = [gen_gp_loss(gp) for gp in model.output_layers]\n",
    "model.compile(optimizer=Adam(1e-2), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
