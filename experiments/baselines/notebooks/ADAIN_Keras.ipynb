{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/patel-zeel/Adhoc/master/ADAIN.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -qq numpy==1.18.5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Concatenate, Lambda, Multiply, Reshape, Input, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import clear_output\n",
    "from time import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', size=16)\n",
    "# import logging\n",
    "# tf.get_logger().setLevel(logging.ERROR)\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAIN(time_window, met_dim):\n",
    "    \"\"\"\n",
    "    # Vocab\n",
    "    met         - meteorology\n",
    "    dis         - distance\n",
    "    aq          - air quality\n",
    "    n_stn       - n_stations\n",
    "    dim         - dimension\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    do = 0.5 # Drop out\n",
    "    np.random.seed(0)\n",
    "    set_seed(0)\n",
    "    \n",
    "    # Defining inputs\n",
    "    train_dist = Input(shape=(2,None,))\n",
    "    train_met_aq = Input(shape=(24,met_dim+1,None,)) # +1 for aq\n",
    "    test_met = Input(shape=(24,met_dim,))\n",
    "    \n",
    "    n_stn = 2 if train_dist.shape[2] is None else train_dist.shape[2]\n",
    "    \n",
    "    # Local station                                                        ## Input                --- Output\n",
    "    lcl_lstm = LSTM(300, dropout=do)(test_met)                             # (-1,time_window,met)  --- (-1,300)\n",
    "    lcl_dns = Dense(200, activation='relu')(lcl_lstm)                      # (-1,300)              --- (-1,200) \n",
    "    lcl_dns = Dropout(do)(lcl_dns)\n",
    "    \n",
    "    # Defininig shared layers (shared among train stations)\n",
    "    s_dns1 = Dense(100, activation='relu')\n",
    "    s_lstm = LSTM(300, activation='relu', dropout=do)\n",
    "    s_dns2 = Dense(200, activation='relu')\n",
    "    \n",
    "    a_dns1 = Dense(200, activation='relu')\n",
    "    a_dns2 = Dense(200)\n",
    "    a_dns3 = Dense(1)\n",
    "    \n",
    "    # Training stations\n",
    "    att_list = [] # Saving station attention weights\n",
    "    stn_list = [] # Saving station features\n",
    "    \n",
    "    # Iterating over train stations using shared layers\n",
    "    for s_i in range(n_stn):                                               ## Input               --- Output\n",
    "        train_dist_slice = Lambda(lambda x:x[:,:,s_i])(train_dist)         # (-1,dis,n_stn)       --- (-1,dis)\n",
    "        stn_dns1 = s_dns1(train_dist_slice)                                # (-1,dis)             --- (-1,100)\n",
    "        stn_dns1 = Dropout(do)(stn_dns1)\n",
    "        train_met_aq_slice = Lambda(lambda x:x[:,:,:,s_i])(train_met_aq)\n",
    "        stn_lstm = s_lstm(train_met_aq_slice)                              # (-1,time_window,met) --- (-1,300)\n",
    "        stn_cat1 = Concatenate()([stn_dns1, stn_lstm])                     # (-1,100)+(-1,300)    --- (-1,400)\n",
    "        stn_dns2 = s_dns2(stn_cat1)                                        # (-1,400)             --- (-1,200)\n",
    "        stn_dns2 = Dropout(do)(stn_dns2)\n",
    "        stn_list.append(stn_dns2)\n",
    "        ### Attention Mechanism\n",
    "        att_cat = Concatenate()([stn_dns2, lcl_dns])                       # (-1,200)*2           --- (-1,400)\n",
    "        att_dns1 = a_dns1(att_cat)                                         # (-1,400)             --- (-1,200)\n",
    "        att_dns1 = Dropout(do)(att_dns1)\n",
    "        att_dns2 = a_dns2(att_dns1)                                        # (-1,200)             --- (-1,200)\n",
    "        att_dns3 = a_dns3(att_dns2)                                        # (-1,200)             --- (-1,1)\n",
    "        att_list.append(att_dns3)\n",
    "    \n",
    "    ### Normalize Attention\n",
    "    att_cat1 = Concatenate()(att_list)                                     # (-1,1)*n_stn         --- (-1,n_stn)\n",
    "    att_cat2 = Lambda(lambda inp: inp/K.sum(inp, axis=0))(att_cat1)        # (-1,n_stn)           --- (-1,n_stn)\n",
    "    \n",
    "    ### Multiply Attention with station features\n",
    "    stn_cat2 = Concatenate()(stn_list)                                     # (-1,200)*n_station   --- (-1,200*n_stn) \n",
    "    stn_cat2 = Lambda(lambda x: K.reshape(x, (-1,200,n_stn)))(stn_cat2)    # (-1,200*n_stn)       --- (-1,200,n_stn)      \n",
    "    att_cat2 = Lambda(lambda x: K.reshape(x, (-1,1,n_stn)))(att_cat2)      # (-1,n_stn)           --- (-1,1,n_stn)\n",
    "#     print(stn_cat2)\n",
    "    stn_mul = Multiply()([stn_cat2, att_cat2])                             # (-1,200,n_stn)*(-1,1,n_stn)  --- (-1,200,n_stn)\n",
    "    stn_add = Lambda(lambda x: K.sum(x, axis=2))(stn_mul)                  # (-1,200,n_stn)       --- (-1,200)\n",
    "    \n",
    "    ### Concatenate local and station features\n",
    "    final_cat = Concatenate()([stn_add, lcl_dns])                          # (-1,200)*2           --- (-1,400)\n",
    "    final_dns1 = Dense(200, activation='relu')(final_cat)                  # (-1,400)             --- (-1,200)\n",
    "    final_dns1 = Dropout(do)(final_dns1)\n",
    "    final_dns2 = Dense(200)(final_dns1)\n",
    "    final_dns3 = Dense(1)(final_dns2)                                      # (-1,200)             --- (-1,1)\n",
    "    \n",
    "    model = Model(inputs=[train_dist, train_met_aq, test_met], outputs=final_dns3)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/50\n",
      "191/191 - 20s - loss: 0.4455 - val_loss: 0.2089\n",
      "Epoch 2/50\n",
      "191/191 - 16s - loss: 0.3423 - val_loss: 0.2124\n",
      "Epoch 3/50\n",
      "191/191 - 16s - loss: 0.2891 - val_loss: 0.1478\n",
      "Epoch 4/50\n",
      "191/191 - 16s - loss: 0.2598 - val_loss: 0.1381\n",
      "Epoch 5/50\n",
      "191/191 - 16s - loss: 0.2367 - val_loss: 0.1137\n",
      "Epoch 6/50\n",
      "191/191 - 16s - loss: 0.2320 - val_loss: 0.1033\n",
      "Epoch 7/50\n",
      "191/191 - 16s - loss: 0.2233 - val_loss: 0.1740\n",
      "Epoch 8/50\n",
      "191/191 - 16s - loss: 0.2079 - val_loss: 0.0928\n",
      "Epoch 9/50\n",
      "191/191 - 16s - loss: 0.2072 - val_loss: 0.1307\n",
      "Epoch 10/50\n",
      "191/191 - 16s - loss: 0.1945 - val_loss: 0.1566\n",
      "Epoch 11/50\n",
      "191/191 - 16s - loss: 0.1832 - val_loss: 0.2772\n",
      "Epoch 12/50\n",
      "191/191 - 16s - loss: 0.1775 - val_loss: 0.2848\n",
      "Epoch 13/50\n",
      "191/191 - 16s - loss: 0.1768 - val_loss: 0.1960\n",
      "Epoch 14/50\n",
      "191/191 - 16s - loss: 0.1774 - val_loss: 0.2704\n",
      "Epoch 15/50\n",
      "191/191 - 16s - loss: 0.1813 - val_loss: 0.2417\n",
      "Epoch 16/50\n",
      "191/191 - 16s - loss: 0.1567 - val_loss: 0.2358\n",
      "Epoch 17/50\n",
      "191/191 - 16s - loss: 0.1657 - val_loss: 0.2213\n",
      "Epoch 18/50\n",
      "191/191 - 16s - loss: 0.1564 - val_loss: 0.2857\n",
      "Epoch 19/50\n",
      "191/191 - 16s - loss: 0.1541 - val_loss: 0.1695\n",
      "Epoch 20/50\n",
      "191/191 - 16s - loss: 0.1598 - val_loss: 0.2651\n",
      "Epoch 21/50\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "set_seed(0)\n",
    "path = '../../../data_and_results/u-air/production/pm25_beijing_best36/quadratic/'\n",
    "n_folds = 6\n",
    "\n",
    "init = time()\n",
    "fold_history = []\n",
    "for fold in [str(i) for i in range(n_folds)]:\n",
    "    clear_output(wait=True)\n",
    "    print('Training fold',fold)\n",
    "    # Load data\n",
    "    train_dst = np.load(path+'data/fold_'+fold+'/train/adain/trn_dst.npy', allow_pickle=True).astype(np.float32)\n",
    "    train_met_aq = np.load(path+'data/fold_'+fold+'/train/adain/trn_metaq.npy').astype(np.float32)\n",
    "    test_met = np.load(path+'data/fold_'+fold+'/train/adain/tst_met.npy').astype(np.float32)\n",
    "    test_aq = np.load(path+'data/fold_'+fold+'/train/adain/tst_aqi.npy').astype(np.float32)\n",
    "    yscaler = pd.read_pickle(path+'data/fold_'+fold+'/scaler/adain/yscaler.pickle')\n",
    "    \n",
    "    # Define and train the model\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "    model = ADAIN(time_window=24, met_dim=4)\n",
    "    history = model.fit(x=[train_dst, train_met_aq, test_met], y=test_aq, \n",
    "          batch_size=32, epochs=50, validation_split=0.2, verbose=2)#, callbacks=[es])\n",
    "    fold_history.append(history)\n",
    "    \n",
    "    # Test the model\n",
    "    train_dst = np.load(path+'data/fold_'+fold+'/test/adain/trn_dst.npy', allow_pickle=True).astype(np.float32)\n",
    "    train_met_aq = np.load(path+'data/fold_'+fold+'/test/adain/trn_metaq.npy').astype(np.float32)\n",
    "    test_met = np.load(path+'data/fold_'+fold+'/test/adain/tst_met.npy').astype(np.float32)\n",
    "    test_aq = np.load(path+'data/fold_'+fold+'/test/adain/tst_aqi.npy').astype(np.float32)\n",
    "    \n",
    "    pred_y = np.nan*np.zeros((train_dst.shape[0], train_dst.shape[-1]))\n",
    "    for test_id in range(train_dst.shape[-1]):\n",
    "        pred_yy = model.predict(x=[train_dst[:,:,:,test_id], train_met_aq, test_met[:,:,:,test_id]])\n",
    "        pred_y[:,test_id] = yscaler.inverse_transform(pred_yy).ravel()\n",
    "    \n",
    "    if not os.path.exists(path+'results/adain/fold_'+fold):\n",
    "        os.makedirs(path+'results/adain/fold_'+fold)\n",
    "    np.save(path+'results/adain/fold_'+fold+'/pred_y.npy', pred_y)\n",
    "    np.save(path+'results/adain/fold_'+fold+'/test_y.npy', test_aq)\n",
    "\n",
    "print('Finished in',round(time()-init)/60,'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../production/pm25_beijing_best36/quadratic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['svr', 'gp_rbf', 'gp_m32', 'gp_m12', 'gp_linear','nsgp_rbf','adain']\n",
    "res = pd.DataFrame(index=models, columns=['fold_'+str(i) for i in range(len(folds))]+['avg'])\n",
    "for model in models[:-1]:\n",
    "    pred_y, test_y = load_results(path, folds, f_ids, n_test, model)\n",
    "    fold_rmse = fold_wise_rmse(pred_y, test_y, len(folds))\n",
    "    fold_rmse.append(np.mean(fold_rmse))\n",
    "    res.loc[model, :] = fold_rmse\n",
    "\n",
    "# ADAIN\n",
    "for fold in folds:\n",
    "    pred_y = np.load(path+'results/adain/fold_'+fold+'/pred_y.npy')\n",
    "    test_y = np.load(path+'results/adain/fold_'+fold+'/test_y.npy')\n",
    "    res.loc['adain', 'fold_'+fold] = mean_squared_error(test_y.ravel(), pred_y.ravel(), squared=False)\n",
    "res.loc['adain','avg'] = res.loc['adain'].mean()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting predictions on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_aq.ravel(), test_aq.ravel())\n",
    "plt.scatter(test_aq.ravel(), pred_y.ravel());\n",
    "plt.xlabel('PM2.5 (Ground truth)');plt.ylabel('PM2.5 (Predictions)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(o_tests, o_tests)\n",
    "plt.plot(test_aq[:,0], label='ground truth');\n",
    "plt.plot(pred_y[:,0], label='predictions');\n",
    "plt.xlabel('Time-stamps'); plt.ylabel('PM2.5');\n",
    "plt.legend(bbox_to_anchor=(1.5,1));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
